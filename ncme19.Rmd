---
title: "Evaluating Content-Related Validity Evidence Using Text Modeling"
shorttitle: "Text Modeling Validity"
date: "`r format(Sys.time(), '%B %d, %Y')`"

author: 
  - name          : "Daniel Anderson"
    affiliation   : "1"
    corresponding : yes
    email         : "daniela@uoregon.edu"
    address       : "5262 University of Oregon"
  - name          : "Brock Rowley"
    affiliation   : "1"
    email         : "brockr@uoregon.edu"
  - name          : "Sondra Stegenga"
    affiliation   : "1"
    email         : "sondras@uoregon.edu"

affiliation:
  - id            : "1"
    institution   : "University of Oregon"

bibliography      : refs.bib

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

abstract: |
  Topic modeling is applied with science content standards to
  evaluate semantic clustering. The probability that each item from a statewide
  assessment belongs to each cluster/topic is then estimated as a source of
  content-related validity evidence. We also show how visualizations can map 
  the content coverage of the test.
class             : "doc"
output            : papaja::apa6_pdf
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE)
```


## Conceptual Framework
Content-related validity evidence is a critical component of the "overall
evaluative judgment" [@messick95, p. 741] of the validity of test scores for a
given use, and is one of the five major sources of validity evidence
outlined by the *Standards for Educational and Psychological Testing* 
[@standards14]. Empirical evaluations of of content validity evidence for
high-stakes tests generally come in the form of alignment studies, with panels
of experts judging the alignment match between the content represented
within the test items and the content represented in the corresponding
standards [@sireci07; @webb97]. In this presentation, we explore the use of
text-mining procedures to evaluate the language used within the content
standards and the corresponding language used in the test items (i.e., the item
stems and response options). We demonstrate that these procedures can be
successfully applied to not only identify correspondence between items and
standards, but also to evaluate the content coverage and standards
representation across the items. 

<!-- 
Archiving B-Rock's stuff because I like it and might want to integrate it back
into the text above.

  The alignment of test items to content standards is critical to content validity. Generally, alignment studies are used to determine content validity for standards-based assessments.  Alignment studies are difficult to execute and design, even under ideal conditions, and present researchers with potential sources of error. The chosen methodology, number of participants, professional judgments, consensus through discussion or averaged by ratings, costs associated with travel, and time are elements for consideration. Text mining may be an additional source of content validity, used to match content standards to items for standards-based assessments.  -->

## Methods 
Our particular application corresponds to evaluating the content-validity
evidence for the statewide alternate assessment based on alternate achievement
standards (AA-AAS) for student with the most significant cognitive 
disabilities [@usdoe05] in one western state. We evaluate the concurrence
between the text in the Grade 8 *Next Generation Science Standards* (NGSS) and
the text in the item stems and response options for the corresponding statewide
AA-AAS. As @ysseldyke97 note, "There is more variability in the skill levels 
and needs of this 1% of the students than there is in the rest of the total
student population" (p. 16). Correspondingly, the development of items followed
a staged process, where content standards were first identified, and then
three versions of essentially the same item were developed to be of,
theoretically, *low*, *medium*, and *high* difficulty. In science, key
vocabulary is a critical component of demonstrating knowledge and all *high* items were written to include this vocabulary. However, this was not
the case for the *low* or *medium* items and we therefore presumed, a priori,
that the textual match of the *high* items with the content standards would be
greater than the *low* or *medium* items.

<!-- The above is probs too much detail for the proposal -->

In evaluating the concordance between the language used in the content
standards and the language used in the test items, our approach is to use a
text-based machine learning model, specifically topic modeling **(CITE)**, to
mine the standards and evaluate the topics represented therein. Once this model
is trained, we can estimate the probability that each item is represented by
each topic. In other words, the model learns the patterns of words from the
standards, and we can then evaluate whether the words used in the items
correspond to those patterns. 

Topic modeling is akin to exploratory factor analysis, where word clusters 
(topics) are determined based on their semantic coherence **(CITE)**. Post-hoc
investigations of the topics then provide substantive meaning. In our
investigation, we expected seven topics to emerge, which generally correspond 
to the sub-domains represented in the Grade 8 NGSS standards.

Latent Dirichlet Allocation (LDA) was used... **(CITE)**. We removed stop words
according to... **(CITE)**.


Data were analyzed using using R [@r] with the *textmodeling* package 
[@grun11]. All code used for the analyses will be made publicly available.

## Preliminary Results

## Conclusions and Implications
Overall, this paper will discuss a new and innovative proposed approach to establishing validity through analyzing and categorizing text data via modern data tools of R and RStudio text analysis and structural topic modeling. It does not replace current methods for establishing validity but demonstrates initial promise to add strength to the development design. In a world of constantly evolving and growing data and data sources it is imperative as educational researchers that we not only begin to explore new methods that hold promise for increasing efficiency and accuracy with data analysis but also ensure that methods engage an element of translatability. By increasing the accuracy of constructs and improved validity we provide the opportunity to increase utility and translatability over a range of consumers in the educational community. In addition, modern technology provides an array of methods and open source resources and tools, such as R and RStudio, that not only provide the ability to capture and categorize the data but also visualize the findings. Again, this is an imperative piece in a world that has seen vastly increased calls for the translation of data and research to practice and practice to research.


\newpage

# References

```{r modeling}
library(tidyverse)
library(rio)
library(here)
library(janitor)
library(tidytext)
library(topicmodels)
#library(fmsb)

standards <- import(here("data", "8gradescience.xlsx"), 
                    setclass = "tbl_df") %>% 
  clean_names()

webbwords <- import(here("data", "stopwords-webb.xlsx"),
                    setclass = "tbl_df")

items <- import(here("data", "G8_Sci_Items.xlsx"),
                setclass = "tbl_df") %>% 
  clean_names()

# Create the document term matrix
dtm <- standards %>% 
  select(domain, ngss_standard) %>% 
  unnest_tokens(word, ngss_standard) %>% 
  anti_join(stop_words) %>% 
  anti_join(webbwords) %>% 
  filter(word != "explanation", # Filter out any additional words we don't want in there
         word != "results") %>% 
  group_by(domain) %>% 
  count(word) %>% 
  ungroup() %>% 
  cast_dtm(domain, word, n)

# Code below produces the bar charts showing most important words within each 
# topic 
#
# tm <- LDA(dtm, k = 7, control = list(seed = 1234)) %>% 
#   tidy()
# 
# top_terms <- tm %>%
#   group_by(topic) %>%
#   top_n(5, beta) %>%
#   ungroup() %>%
#   arrange(topic, -beta)
# 
# top_terms %>%
#   mutate(term = reorder(term, beta)) %>%
#   ggplot(aes(term, beta, fill = factor(topic))) +
#   geom_col(alpha = 0.8, show.legend = FALSE) +
#   facet_wrap(~topic, scales = "free_y") +
#   coord_flip()

# Train the model
tm_raw <- LDA(dtm, k = 7, control = list(seed = 1234))

# Create a document term matrix for items 
idtm <- items %>% 
  select(item_id, prompt) %>% 
  unnest_tokens(word, prompt) %>% 
  anti_join(stop_words) %>% 
  anti_join(webbwords) %>% 
  group_by(item_id) %>% 
  count(word) %>% 
  ungroup() %>% 
  cast_dtm(item_id, word, n)

# Predict the topic for each item
posteriors <- posterior(tm_raw, newdata = idtm)

# NOTE: To create the radar charts, you may need to keep it in this format
# I'm note sure. The below is a good ggplot format.

probs <- posteriors$topics %>% 
  as.data.frame() %>% 
  mutate(item = rownames(.)) %>% 
  tbl_df() %>% 
  gather(topic, probability, -item)

# Less than ideal ggplot solution with coord_polar
theme_set(theme_minimal())
probs %>% 
  mutate(topic = as.numeric(topic),
         level = str_extract(item, "L|M|H"),
         level = factor(level, 
                        levels = c("L", "M", "H"))) %>% 
  ungroup() %>% 
  ggplot(aes(topic, probability, group = item)) +
    geom_line(lwd = 1.2, alpha = 0.2, color = "cornflowerblue") +
    geom_point(color = "gray60", alpha = 0.2) +
    coord_polar() +
    scale_color_viridis_d() +
    facet_wrap(~level) +
    guides(color = "none")
```